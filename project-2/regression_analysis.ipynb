{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import scipy\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./datasets/wdbc.data\", header=None).drop(columns=[0])\n",
    "df_benign = df[df[1] == 'B'].drop(columns=[1])\n",
    "\n",
    "col_names = [\n",
    "    \"radius\",\n",
    "    \"texture\",\n",
    "    \"perimeter\",\n",
    "    \"area\",\n",
    "    \"smoothness\",\n",
    "    \"compactness\",\n",
    "    \"concavity\",\n",
    "    \"concave\",\n",
    "    \"symmetry\",\n",
    "    \"fractal\"\n",
    "]\n",
    "columns = list(range(2, 12))\n",
    "columns_dict = dict(zip(columns, col_names))\n",
    "\n",
    "df_benign = df_benign[columns].rename(columns=columns_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_benign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perfect_linear_corr_set = ['radius', 'perimeter']\n",
    "biased_linear_corr_set = ['compactness', 'concavity']\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "df_linear = df_benign[perfect_linear_corr_set].sort_values(by=perfect_linear_corr_set[0]).reset_index(drop=True)\n",
    "df_biased_linear = df_benign[biased_linear_corr_set].sort_values(by=biased_linear_corr_set[0]).reset_index(drop=True)\n",
    "\n",
    "def display_linear(ax, ax_i, df_linear):\n",
    "    ax[ax_i].scatter(df_linear.values[:, 0], df_linear.values[:, 1])\n",
    "    ax[ax_i].set_title(\"Almost perfect linear correlation\")\n",
    "    ax[ax_i].set_xlabel(perfect_linear_corr_set[0])\n",
    "    ax[ax_i].set_ylabel(perfect_linear_corr_set[1])\n",
    "    x_min, x_max = df_linear.values[:, 0].min(), df_linear.values[:, 0].max()\n",
    "    y_min, y_max = df_linear.values[:, 1].min(), df_linear.values[:, 1].max()\n",
    "    ax[ax_i].set_xticks(np.arange(np.floor(x_min), np.ceil(x_max) + 1, 1))\n",
    "    ax[ax_i].set_yticks(np.arange(np.floor(y_min), np.ceil(y_max) + 1, 2))\n",
    "\n",
    "def display_biased_linear(ax, ax_i, df_biased_linear):\n",
    "    ax[ax_i].scatter(df_biased_linear.values[:, 0], df_biased_linear.values[:, 1])\n",
    "    ax[ax_i].set_title(\"Biased linear correlation\")\n",
    "    ax[ax_i].set_xlabel(biased_linear_corr_set[0])\n",
    "    ax[ax_i].set_ylabel(biased_linear_corr_set[1])\n",
    "    x_min, x_max = df_biased_linear.values[:, 0].min(), df_biased_linear.values[:, 0].max()\n",
    "    y_min, y_max = df_biased_linear.values[:, 1].min(), df_biased_linear.values[:, 1].max()\n",
    "    ax[ax_i].set_xticks(np.linspace(x_min, x_max, num=10))\n",
    "    ax[ax_i].set_yticks(np.linspace(y_min, y_max, num=10))\n",
    "\n",
    "display_linear(ax, 0, df_linear)\n",
    "display_biased_linear(ax, 1, df_biased_linear)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_analyze_model(df, x_col, y_col, biased=False, outlier_threshold=3):\n",
    "    X = sm.add_constant(df[x_col])\n",
    "    y = df[y_col]\n",
    "\n",
    "    model = sm.OLS(y, X)\n",
    "    model_results = model.fit()\n",
    "\n",
    "    print(f\"\\n\\n{\"=\"*10} Correlation: {\"=\"*10}\")\n",
    "    correlation_value = df[x_col].corr(df[y_col], method='pearson')  # ((x - x.mean()) * (y - y.mean())).mean() / (x.std() * y.std())\n",
    "    print(f'Pearson correlation coefficient between `{x_col}` and `{y_col}`: {correlation_value}')\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\n\\n{\"=\"*10} Model summary description: {\"=\"*10}\")\n",
    "    print(model_results.summary())\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    residuals = model_results.resid\n",
    "    fitted_values = model_results.fittedvalues\n",
    "\n",
    "\n",
    "    print(f\"\\n\\n{\"=\"*10} Residuals squared error: {\"=\"*10}\")\n",
    "    rse = np.mean(residuals ** 2)\n",
    "    print(f'RSE: {rse}')\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    print(f\"\\n\\n{\"=\"*10} Residuals statistics description: {\"=\"*10}\")\n",
    "    display(residuals.describe())\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    fig, ax = plt.subplots(3, 3, figsize=(18, 18))\n",
    "    ax = ax.flatten()\n",
    "\n",
    "    if biased:\n",
    "        display_biased_linear(ax, 0, df)\n",
    "    else:\n",
    "        display_linear(ax, 0, df)\n",
    "    xs = np.linspace(df[x_col].min(), df[x_col].max(), 2)\n",
    "    ys = model_results.params[x_col] * xs + model_results.params['const']\n",
    "    ax[0].plot(xs, ys, color='red')\n",
    "    ax[0].set_title(\"Regression line\")\n",
    "\n",
    "    sns.histplot(residuals, kde=True, ax=ax[1])\n",
    "    ax[1].set_title('Histogram residuals')\n",
    "    \n",
    "    scipy.stats.probplot(residuals, dist=\"norm\", plot=ax[2])\n",
    "    ax[2].set_title('Q-Q plot residuals')\n",
    "\n",
    "    ax[3].scatter(fitted_values, residuals, edgecolors='k', facecolors='none')\n",
    "    ax[3].axhline(0, color='gray', linestyle='dashed', linewidth=2)\n",
    "    ax[3].set_title(\"Residuals vs fitted\")\n",
    "    ax[3].set_xlabel(\"Fitted values\")\n",
    "    ax[3].set_ylabel(\"Residuals\")\n",
    "    z = sm.nonparametric.lowess(residuals, fitted_values)\n",
    "    ax[3].plot(z[:, 0], z[:, 1], color='red', lw=2)\n",
    "\n",
    "\n",
    "    sqrt_standardized_residuals = np.sqrt(np.abs(residuals / np.std(residuals)))\n",
    "    ax[4].scatter(fitted_values, sqrt_standardized_residuals, edgecolors='k', facecolors='none')\n",
    "    z = sm.nonparametric.lowess(sqrt_standardized_residuals, fitted_values)\n",
    "    ax[4].plot(z[:, 0], z[:, 1], color='red', lw=2)\n",
    "    ax[4].set_title(\"Scale-location\")\n",
    "    ax[4].set_xlabel(\"Fitted values\")\n",
    "    ax[4].set_ylabel(\"√|Standardized residuals|\")\n",
    "\n",
    "\n",
    "    cooks_d = model_results.get_influence().cooks_distance[0]\n",
    "    outlier_cooks_threshold = (4 / (cooks_d.size - 1))\n",
    "    outliers_cooks_d = np.where(cooks_d > outlier_cooks_threshold)[0]\n",
    "    ax[5].plot(cooks_d, 'bo', linestyle='None')\n",
    "    ax[5].axhline(outlier_cooks_threshold, color='red', linestyle='dashed', label=f\"Threshold: {outlier_cooks_threshold}\")\n",
    "    ax[5].set_title(\"Cook's Distance\")\n",
    "    ax[5].set_xlabel(\"Observation Index\")\n",
    "    ax[5].set_ylabel(\"Cook's Distance\")\n",
    "\n",
    "    standardized_residuals = np.abs(residuals / np.std(residuals))\n",
    "    ax[6].boxplot(standardized_residuals)\n",
    "    ax[6].set_title('Boxplot of Standardized Residuals')\n",
    "    ax[6].set_ylabel('Standardized Residuals')\n",
    "\n",
    "    # Wartości odstające i wpływowe\n",
    "    sm.graphics.influence_plot(model_results, criterion=\"cooks\", ax=ax[7])\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(f\"\\n\\n{\"=\"*10} Residual normality check: {\"=\"*10}\")\n",
    "    print(\"Shapiro-Wilk test p-value:\", scipy.stats.shapiro(residuals)[1])\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    print(f\"\\n\\n{\"=\"*10} Residual skewness check: {\"=\"*10}\")\n",
    "    print(\"Skewness:\", pd.Series(residuals).skew())\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    print(f\"\\n\\n{\"=\"*10} Residual kurtosis check: {\"=\"*10}\")\n",
    "    print(\"Kurtosis:\", pd.Series(residuals).kurtosis())\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    \n",
    "    # Homoskedastyczność (test Levene'a)\n",
    "    print(f\"\\n\\n{\"=\"*10} Homoscedasticity check: {\"=\"*10}\")\n",
    "    median = np.median(fitted_values)\n",
    "    group1 = residuals[fitted_values <= median]\n",
    "    group2 = residuals[fitted_values > median]\n",
    "    levene_test = scipy.stats.levene(group1, group2)\n",
    "    print(\"Levene's test p-value:\", levene_test.pvalue)\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Test Box-Pierce\n",
    "    print(f\"\\n\\n{\"=\"*10} Autocorrelation check (for lags=1,2,3): {\"=\"*10}\")\n",
    "    boxpierce_results = sm.stats.diagnostic.acorr_ljungbox(residuals, lags=3, boxpierce=True)\n",
    "    display(boxpierce_results)\n",
    "    print(\"Box-Pierce test statistic:\", boxpierce_results['bp_stat'].values)\n",
    "    print(\"Box-Pierce test p-value:\", boxpierce_results['bp_pvalue'].values)\n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n",
    "    print(f\"\\n\\n{\"=\"*10} Outliers list: {\"=\"*10}\")\n",
    "    print(f\"\\nIndices of outliers based on cooks distance (beyond threshold {outlier_cooks_threshold}): {outliers_cooks_d}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    print(f\"\\n\\n{\"=\"*10} Correlation coefficient statistical significance: {\"=\"*10}\")\n",
    "    r2 = model_results.rsquared\n",
    "    stat = r2 / np.sqrt((1 - r2) / (df.shape[0]-2))\n",
    "    crit = scipy.stats.t.ppf(1 - 0.05/2, df.shape[0]-2)\n",
    "    print(f\"Statystyka t: {stat} \\nWartość krytyczna (górna): {crit}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return outliers_cooks_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h4>\n",
    "Badanie liniowej zależnosci miedzy `radius` a `perimeter`\n",
    "</h4>\n",
    "</center>\n",
    "\n",
    "Wiemy, że zależność miedzy promieniem a obwodem jest liniowa z wzoru 2PIr. Oczekujemy prawie perfekcyjnej liniowej zależności, która będzie \n",
    "zaburzona najprawdopodobnije błędami pomiaru. Promień może być jedynie dodatni oraz jego wielkość również nie moze przekraczać, powiedzmy 40cm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_linear = df_linear[(df_linear.radius > 0) & (df_linear.radius < 40)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = build_and_analyze_model(df_linear, 'radius', 'perimeter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Korelacja wynosi 0.99 co świadczy o bardzo wysokiej zależności między tymi dwoma zmiennymi. Dopasowany model ma postać funkcyjną \\\n",
    "y = 6.61 * x + -2.21 \\\n",
    "i został dopasowany na 357 obserwacjach. \n",
    "- Wartości reziduów mają rozkład normalny, prawoskośny. O normalności świadczy test saphiro-wilka,\n",
    "którego p-value < 0.05 (qq-plot wykazuje minimalne odchylenie od normalności, ponieważ nie wsyzskite wartości leżą ideealnie na linii.)\n",
    "- Dodatnie Skewness (1.32) wskazuje na prawostronną skośność, co widoczne jest też na histogramie - sporo predykcji dla których otrzymujemy zbyt duze wartości. Kurtoza ma wartość 4 co oznaczałoby\n",
    "grube ogony w rozkladzie reziduów. \\\n",
    "- Rozklad reziduów ma mean~0 oraz std~1\n",
    "- Dla `const` i `radius` mamy hipotezy zerowe H_0 mówiące o tym, że współczynnik jest równy zero. Dla obu wartości t nie wpada w 95%-przedział\n",
    "ufności, zatem odrzucamy takie hipotezy na rzecz hipotezy H_1 mówiącej o tym, że współczynnik nie jest równy 0. OBA WSPÓŁCZYNNIKI SĄ ISTOTNE STATYSTYCZNIE.\n",
    "- RSE wynosi 0.8969853117760591 co oznacza, że średni błąd kwadratowy jest równy 0.8969853117760591. Jest to akceptowalna wartość, ponieważ opeujemy na dużym zakresie wartości (ok 44-100)\n",
    "- **Model wyjaśnia 99% wariancji zmiennej y przez zmienną x** ponieważ R^2 = 0.99\n",
    "- p-value dla F-statistic jest równe 0 (< 0.05), co interpretujemy jako niemożliwość wyjaśnienia wartości y za pomocą wzoru y = b. Musimy użyć y = ax + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Z wykresu Residuals vs Fitted wynika, że nie zachodzi trend w rozkladzie wartosci reziduów. Wykres uklada sie równomiernie\n",
    "wokół prostej y=0\n",
    "- Test Levene'a wykazuje p-value < 0.05, co mówiłoby o homoskedastyczności reziduów. Oznacza to, że mamy stałe wartości reszt wzdłuż\n",
    "wartości przewidywanych przez model. Rozrzut wartości reszt wokół linii regresji jest stały na wszystkich wartościach x (dla róznych przedziałów).\n",
    "- Wykres Scale-location informuje nas, ze pierwiastek z ustandaryzowanych reszt jest praktycznie zawsze stały dla wszystkich przedziałów (wartości)\n",
    "zmiennej niezależnej X (tj. dla wszystkich dopasowanych wartości y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nie występuje autokorelacja reszt składnika losowego (reszty nie są ze sobą skorelowane), ponieważ nie odrzucamy hipotezy H_0\n",
    "mówiącej o niewystępowaniu autokorelacji. Na poziomach lag=1,2,3 otrzymujemy p-value z testu Boxa-Pierca: [0.08561466 0.17790812 0.15246615]\n",
    "które są >0.05. Zatem potwierdzamy H_0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Do nastepnych eksperymentów odrzucamy obserwacje znaczące (wpływowe), tj. dla których odległość cook'a była większa od 0.011235955056179775 (na podstawie heurystyki 4/(n-1))\n",
    "\n",
    "- Statystyka t jest większa od wartości krytycznej dla n-2 stopni swobody, gdzie n jest liczbą obserwacji. Odrzucamy hipotezę zerową, \n",
    "że korelacja jest równa 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h5>\n",
    "Teraz wykonamy eksperymenty dla tych samych zmiennych, ale z odfiltrowanymi obserwacjami wpływowymi na podstawie odległości cooka\n",
    "</h5>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_linear_filtered = df_linear.drop(outliers)\n",
    "build_and_analyze_model(df_linear_filtered, 'radius', 'perimeter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak widać :\n",
    "- QQ plot przyjął teraz bardziej liniowy kształt, \n",
    "- histogram reszt nie jest juz prawoskośny (nie ma obserwacji odstających w prawym ogonie),\n",
    "- influence plot nie posiada juz tak zróżnicowanych wartości na osi X\n",
    "- box plot nie ma tak zróżnicowanych wartości, chociaz jeszcze zostały 3 outliery, \n",
    "- skala w Residuals vs fitted nie jest już tak zróżnicowana (największe wartości to 2, a nie 6 jak poprzednio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def cross_validate_ols(df, x_cols, y_col, n_splits=10):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    r2_scores = []\n",
    "    rse_scores = []\n",
    "\n",
    "    for train_index, test_index in kf.split(df):\n",
    "        X_train, X_test = df[x_cols].iloc[train_index], df[x_cols].iloc[test_index]\n",
    "        y_train, y_test = df[y_col].iloc[train_index], df[y_col].iloc[test_index]\n",
    "\n",
    "        X_train = sm.add_constant(X_train)\n",
    "        model = sm.OLS(y_train, X_train)\n",
    "        model_results = model.fit()\n",
    "\n",
    "        X_test = sm.add_constant(X_test)\n",
    "        y_pred =  (model_results.params[x_cols[0]] * X_test[x_cols[0]] +  model_results.params['const']).values.reshape(-1)\n",
    "        y_test = y_test.values.reshape(-1)\n",
    "\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        r2_scores.append(r2)\n",
    "\n",
    "        residuals = y_test - y_pred\n",
    "\n",
    "        rse = np.mean(residuals ** 2)\n",
    "        rse_scores.append(rse)\n",
    "\n",
    "    mean_r2 = np.mean(r2_scores)\n",
    "    mean_rse = np.mean(rse_scores)\n",
    "\n",
    "    return mean_r2, mean_rse\n",
    "\n",
    "def plot_train_test_reg(df, x_cols, y_col):\n",
    "    X = df[x_cols]\n",
    "    y = df[y_col]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "    X_train = sm.add_constant(X_train)\n",
    "    model = sm.OLS(y_train, X_train)\n",
    "    model_results = model.fit()\n",
    "    xs = np.linspace(X_train[x_cols[0]].min(), X_train[x_cols[0]].max(), 2)\n",
    "    ys = model_results.params[x_cols[0]] * xs + model_results.params['const']\n",
    "    \n",
    "    plt.scatter(X_train[x_cols[0]], y_train, color='blue', label='Train')\n",
    "    plt.scatter(X_test, y_test, color='red', label='Test')\n",
    "    plt.plot(xs, ys, color='purple')\n",
    "    plt.xlabel(x_cols)\n",
    "    plt.ylabel(y_col)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_r2, cv_rse = cross_validate_ols(df_linear_filtered, ['radius'], ['perimeter'])\n",
    "print(f\"R2 for 10-fold cross validation: {cv_r2} \\nRSE for 10-fold cross validation: {cv_rse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_test_reg(df_linear_filtered, ['radius'], ['perimeter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h4>\n",
    "Badanie liniowej zależnosci miedzy `compactness` a `concavity`\n",
    "</h4>\n",
    "</center>\n",
    "\n",
    "W tym zbiorze widac pewną korelacje obu zmiennych i chcemy sprawdzić, czy można na nim sensownie dopasować model regresji liniowej.\n",
    "Oczekujemy umiarkowanej liniowej zależności ze sporymi reziduami. Są to wartosci, które powinny przyjmowac wartości dodatnie więc \n",
    "przyjmujemy `comcpactness` > 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_biased_linear = df_biased_linear[(df_biased_linear.compactness > 0)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = build_and_analyze_model(df_biased_linear, 'compactness', 'concavity', biased=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h5>\n",
    "Teraz wykonamy eksperymenty dla tych samych zmiennych, ale z odfiltrowanymi obserwacjami wpływowymi na podstawie odległości cooka\n",
    "</h5>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_biased_linear_filtered = df_biased_linear.drop(outliers)\n",
    "build_and_analyze_model(df_biased_linear_filtered, 'compactness', 'concavity', biased=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_r2, cv_rse = cross_validate_ols(df_biased_linear_filtered, ['compactness'], ['concavity'])\n",
    "print(f\"R2 for 10-fold cross validation: {cv_r2} \\nRSE for 10-fold cross validation: {cv_rse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_test_reg(df_biased_linear_filtered, ['compactness'], ['concavity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
